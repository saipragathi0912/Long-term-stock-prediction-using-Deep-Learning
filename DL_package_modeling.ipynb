{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AvcqlypU1yYo",
        "Xv7eLTQuH-8s",
        "trbqENdrKyBF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Installing and Importing Libraries"
      ],
      "metadata": {
        "id": "7V68NJWmMVR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2VWUbMpDCdr",
        "outputId": "833893d2-bdb9-4fb5-abc5-96116f2ad451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.31.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.51.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKJ_oP7gRYAa",
        "outputId": "dd2a59ab-93d8-4ffe-f806-3f2e7d635c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout,Activation, Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from numpy.random import seed\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pickle"
      ],
      "metadata": {
        "id": "hMvBoggvErbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model - 1 :\n",
        "This is a simple binary classification(Up/Down) model built only using Balance sheet "
      ],
      "metadata": {
        "id": "AvcqlypU1yYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  file = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Processed_data/balance_sheet_filtered_augmented_data.txt'\n",
        "  model_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Models/model1.pkl'\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  pos = 0\n",
        "  neg = 0\n",
        "  pad_token=0.0\n",
        "  with open(file, 'r') as f:\n",
        "    next(f)\n",
        "    content = f.read().split('\\n')\n",
        "    for row in content:\n",
        "      data = row.split(',')\n",
        "      if len(data) > 4:\n",
        "        if float(data[4]) > 0:\n",
        "          y.append(True)\n",
        "          pos += 1\n",
        "        else:\n",
        "          y.append(False)\n",
        "          neg += 1\n",
        "        X.append([0.0 if not x else float(x) for x in data[5:]])\n",
        "\n",
        "  print(pos, neg, 1.0 * max(pos, neg) / (pos + neg))\n",
        "  x_padded = list(zip(*itertools.zip_longest(*X, fillvalue=pad_token)))\n",
        "  X=[list(i) for i in x_padded]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "  \n",
        "  #X_train = np.asarray(X_train).astype(np.float32)\n",
        "  #Y_train = np.asarray(Y_train).astype(np.float32)\n",
        "  model = None\n",
        "  if os.path.exists(model_path):\n",
        "    model=pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Models/model1.pkl', 'rb'))\n",
        "  else:\n",
        "    input_dim = len(X_train[0])\n",
        "    model = tf.keras.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "      tf.keras.layers.Dense(\n",
        "        1024,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        # kernel_regularizer='l2',\n",
        "      ),\n",
        "      tf.keras.layers.Dense(\n",
        "        1024,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        # kernel_regularizer='l2',\n",
        "      ),\n",
        "      tf.keras.layers.Dense(\n",
        "        1,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        # kernel_regularizer='l2',\n",
        "      ),\n",
        "    ])\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(\n",
        "      optimizer='adam',\n",
        "      loss=loss_fn,\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "    model.fit(X_train, y_train, epochs=500, batch_size=32)\n",
        "    pickle.dump(model, open(model_path, 'wb'))\n",
        "\n",
        "  print(model.evaluate(X_train, y_train))\n",
        "  print(model.evaluate(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA9496StMOqp",
        "outputId": "940523a8-ad73-49a9-d88f-06374dec036e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 210 0.5121951219512195\n",
            "Keras model archive loading:\n",
            "File Name                                             Modified             Size\n",
            "config.json                                    2023-03-22 11:42:04         2171\n",
            "variables.h5                                   2023-03-22 11:42:04     13788296\n",
            "metadata.json                                  2023-03-22 11:42:04           64\n",
            "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
            "...layers\n",
            "......dense\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_1\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_2\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...metrics\n",
            "......binary_accuracy\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......mean\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...optimizer\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            ".........10\n",
            ".........11\n",
            ".........12\n",
            ".........2\n",
            ".........3\n",
            ".........4\n",
            ".........5\n",
            ".........6\n",
            ".........7\n",
            ".........8\n",
            ".........9\n",
            "...vars\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 4.2971e-06 - binary_accuracy: 1.0000\n",
            "[4.297058694646694e-06, 1.0]\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 3.8951e-06 - binary_accuracy: 1.0000\n",
            "[3.895068402925972e-06, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model - 2 : \n",
        "This is a simple binary classification model built only using cash flow statements"
      ],
      "metadata": {
        "id": "Xv7eLTQuH-8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  file = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Processed_data/cash_flow_filtered_augmented_data.txt'\n",
        "  model_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Models/model2.pkl'\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  pos = 0\n",
        "  neg = 0\n",
        "  with open(file, 'r') as f:\n",
        "    next(f)\n",
        "    content = f.read().split('\\n')\n",
        "    for row in content:\n",
        "      data = row.split(',')\n",
        "      if len(data) > 4:\n",
        "        if float(data[4]) > 0:\n",
        "          y.append(True)\n",
        "          pos += 1\n",
        "        else:\n",
        "          y.append(False)\n",
        "          neg += 1\n",
        "        X.append([0.0 if not x else float(x) for x in data[5:]])\n",
        "\n",
        "  print(pos, neg, 1.0 * max(pos, neg) / (pos + neg))\n",
        "  x_padded = list(zip(*itertools.zip_longest(*X, fillvalue=pad_token)))\n",
        "  X=[list(i) for i in x_padded]\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  model = None\n",
        "  if os.path.exists(model_path):\n",
        "    model=pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Models/model2.pkl', 'rb'))\n",
        "  else:\n",
        "    input_dim = len(X_train[0])\n",
        "    model = tf.keras.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "      tf.keras.layers.Dense(\n",
        "        1024,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        kernel_regularizer='l2',\n",
        "      ),\n",
        "      tf.keras.layers.Dense(\n",
        "        1024,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        kernel_regularizer='l2',\n",
        "      ),\n",
        "      tf.keras.layers.Dense(\n",
        "        1,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        kernel_regularizer='l2',\n",
        "      ),\n",
        "    ])\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(\n",
        "      optimizer='adam',\n",
        "      loss=loss_fn,\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=500, batch_size=32)\n",
        "    pickle.dump(open(model_path,'wb'))\n",
        "\n",
        "  print(model.evaluate(X_train, y_train))\n",
        "  print(model.evaluate(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiKVtFnkGRC_",
        "outputId": "abcb3795-0d35-43d1-d673-77196f1f3833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "195 195 0.5\n",
            "Keras model archive loading:\n",
            "File Name                                             Modified             Size\n",
            "config.json                                    2023-03-22 11:53:10         2339\n",
            "variables.h5                                   2023-03-22 11:53:10     13456520\n",
            "metadata.json                                  2023-03-22 11:53:10           64\n",
            "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
            "...layers\n",
            "......dense\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_1\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_2\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...metrics\n",
            "......binary_accuracy\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......mean\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...optimizer\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            ".........10\n",
            ".........11\n",
            ".........12\n",
            ".........2\n",
            ".........3\n",
            ".........4\n",
            ".........5\n",
            ".........6\n",
            ".........7\n",
            ".........8\n",
            ".........9\n",
            "...vars\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3430 - binary_accuracy: 1.0000\n",
            "[0.34303972125053406, 1.0]\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.3400 - binary_accuracy: 1.0000\n",
            "[0.34000566601753235, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model - 3\n",
        "This is a simple binary classification model built only using income statements"
      ],
      "metadata": {
        "id": "trbqENdrKyBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  file = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Processed_data/income_flow_filtered_augmented_data.txt'\n",
        "  model_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Models/model3.pkl'\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  pos = 0\n",
        "  neg = 0\n",
        "  with open(file, 'r') as f:\n",
        "    next(f)\n",
        "    content = f.read().split('\\n')\n",
        "    for row in content:\n",
        "      data = row.split(',')\n",
        "      if len(data) > 4:\n",
        "        if float(data[4]) > 0:\n",
        "          y.append(True)\n",
        "          pos += 1\n",
        "        else:\n",
        "          y.append(False)\n",
        "          neg += 1\n",
        "        X.append([0.0 if not x else float(x) for x in data[5:]])\n",
        "\n",
        "  print(pos, neg, 1.0 * max(pos, neg) / (pos + neg))\n",
        "  x_padded = list(zip(*itertools.zip_longest(*X, fillvalue=pad_token)))\n",
        "  X=[list(i) for i in x_padded]\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "  model = None\n",
        "  if os.path.exists(model_path):\n",
        "    model = pickle.load(open(model_path,'rb'))\n",
        "  else:\n",
        "    input_dim = len(X_train[0])\n",
        "    model = tf.keras.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "      tf.keras.layers.Dense(\n",
        "        1024,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        # kernel_regularizer='l2',\n",
        "      ),\n",
        "      tf.keras.layers.Dense(\n",
        "        1024,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        # kernel_regularizer='l2',\n",
        "      ),\n",
        "      tf.keras.layers.Dense(\n",
        "        1,\n",
        "        activation='sigmoid',\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "        # kernel_regularizer='l2',\n",
        "      ),\n",
        "    ])\n",
        "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(\n",
        "      optimizer='adam',\n",
        "      loss=loss_fn,\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=500, batch_size=32)\n",
        "    pickle.dump(model,open(model_path,'wb'))\n",
        "  print(model.evaluate(X_train, y_train))\n",
        "  print(model.evaluate(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW2CYG57LD_L",
        "outputId": "f53942bc-1510-46fa-a605-36e1549c9537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "385 390 0.5032258064516129\n",
            "Keras model archive loading:\n",
            "File Name                                             Modified             Size\n",
            "config.json                                    2023-03-22 11:59:58         2171\n",
            "variables.h5                                   2023-03-22 11:59:58     13321352\n",
            "metadata.json                                  2023-03-22 11:59:58           64\n",
            "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
            "...layers\n",
            "......dense\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_1\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dense_2\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...metrics\n",
            "......binary_accuracy\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......mean\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...optimizer\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            ".........10\n",
            ".........11\n",
            ".........12\n",
            ".........2\n",
            ".........3\n",
            ".........4\n",
            ".........5\n",
            ".........6\n",
            ".........7\n",
            ".........8\n",
            ".........9\n",
            "...vars\n",
            "22/22 [==============================] - 0s 2ms/step - loss: 4.5618e-07 - binary_accuracy: 1.0000\n",
            "[4.561805724279111e-07, 1.0]\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 4.2467e-07 - binary_accuracy: 1.0000\n",
            "[4.246720379796898e-07, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model - 4\n",
        "LSTM model for multiclass classification using union of the statements\n"
      ],
      "metadata": {
        "id": "Mw7kz7HuMcn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  input_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Processed_data/combined_filtered_data_union.txt'\n",
        "  model_path = '/content/drive/MyDrive/Colab Notebooks/Deep Learning Package/Models/model4.pkl'\n",
        "  \n",
        "  symbol_vec_map = {}\n",
        "  with open(input_path, 'r') as f:\n",
        "    next(f)\n",
        "    content = f.read().split('\\n')\n",
        "    for row in content:\n",
        "      if not row:\n",
        "        continue\n",
        "\n",
        "      tokens = row.split(',')\n",
        "      if tokens[0] not in symbol_vec_map:\n",
        "        symbol_vec_map[tokens[0]] = []\n",
        "      try:\n",
        "        tokens.append(datetime.strptime(tokens[1], '%Y-%m-%d'))\n",
        "        symbol_vec_map[tokens[0]].append(tokens[1:])\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  pad_token=0.0\n",
        "  for symbol in symbol_vec_map:\n",
        "    symbol_vec_map[symbol].sort(key=lambda x: x[-1])\n",
        "    #print(symbol_vec_map[symbol][0][])\n",
        "    for i in range(len(symbol_vec_map[symbol]) - 1):\n",
        "      #print(symbol_vec_map[symbol][0][4:-1])\n",
        "      #prev_data = symbol_vec_map[symbol][i][4:-1]\n",
        "      curr_data = symbol_vec_map[symbol][i+1][4:-1]\n",
        "      #print(prev_data)\n",
        "      X.append(\n",
        "        [0.0 if not x else float(x) for x in curr_data]\n",
        "      )\n",
        "\n",
        "      value = float(symbol_vec_map[symbol][i+1][3])\n",
        "\n",
        "      if value >= 100:\n",
        "        y.append([0,0,0,0,0,1])\n",
        "      elif value >= 50:\n",
        "        y.append([0,0,0,0,1,0])\n",
        "      elif value >= 0:\n",
        "        y.append([0,0,0,1,0,0])\n",
        "      elif value >= -50:\n",
        "        y.append([0,0,1,0,0,0])\n",
        "      elif value >= -100:\n",
        "        y.append([0,1,0,0,0,0])\n",
        "      else:\n",
        "        y.append([1,0,0,0,0,0])\n",
        "\n",
        "  op=[]\n",
        "  x_padded = list(zip(*itertools.zip_longest(*X, fillvalue=pad_token)))\n",
        "  X=[list(i) for i in x_padded]\n",
        "  print(np.array(X).shape)\n",
        "  print(np.array(y).shape)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  model = None\n",
        "  if os.path.exists(model_path):\n",
        "    print('~~~~~ reusing existing model')\n",
        "    model = pickle.load(open(model_path,'rb'))\n",
        "  else:\n",
        "    print('~~~~~ creating new model')\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units = 50, return_sequences = True, input_shape = (np.array(X_train).shape[1], 1)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units = 50, return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units = 50, return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units = 50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units = 1))\n",
        "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "    print(model.summary())\n",
        "    print('~~~~~ start model fit')\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=500, batch_size=32)\n",
        "    pickle.dump(model,open(model_path,'wb'))\n",
        "\n",
        "  print(model.evaluate(X_train, y_train))\n",
        "  print(model.evaluate(X_test, y_test))"
      ],
      "metadata": {
        "id": "8M72sMXVNkIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5682aa0d-f6cd-4cb7-e719-7040d359f579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 215)\n",
            "(120, 6)\n",
            "~~~~~ reusing existing model\n",
            "Keras model archive loading:\n",
            "File Name                                             Modified             Size\n",
            "variables.h5                                   2023-03-23 15:18:24       901528\n",
            "config.json                                    2023-03-23 15:18:24         4899\n",
            "metadata.json                                  2023-03-23 15:18:24           64\n",
            "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
            "...layers\n",
            "......dense\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "......dropout\n",
            ".........vars\n",
            "......dropout_1\n",
            ".........vars\n",
            "......dropout_2\n",
            ".........vars\n",
            "......dropout_3\n",
            ".........vars\n",
            "......lstm\n",
            ".........cell\n",
            "............vars\n",
            "...............0\n",
            "...............1\n",
            "...............2\n",
            ".........vars\n",
            "......lstm_1\n",
            ".........cell\n",
            "............vars\n",
            "...............0\n",
            "...............1\n",
            "...............2\n",
            ".........vars\n",
            "......lstm_2\n",
            ".........cell\n",
            "............vars\n",
            "...............0\n",
            "...............1\n",
            "...............2\n",
            ".........vars\n",
            "......lstm_3\n",
            ".........cell\n",
            "............vars\n",
            "...............0\n",
            "...............1\n",
            "...............2\n",
            ".........vars\n",
            "...metrics\n",
            "......mean\n",
            ".........vars\n",
            "............0\n",
            "............1\n",
            "...optimizer\n",
            "......vars\n",
            ".........0\n",
            ".........1\n",
            ".........10\n",
            ".........11\n",
            ".........12\n",
            ".........13\n",
            ".........14\n",
            ".........15\n",
            ".........16\n",
            ".........17\n",
            ".........18\n",
            ".........19\n",
            ".........2\n",
            ".........20\n",
            ".........21\n",
            ".........22\n",
            ".........23\n",
            ".........24\n",
            ".........25\n",
            ".........26\n",
            ".........27\n",
            ".........28\n",
            ".........3\n",
            ".........4\n",
            ".........5\n",
            ".........6\n",
            ".........7\n",
            ".........8\n",
            ".........9\n",
            "...vars\n",
            "3/3 [==============================] - 1s 28ms/step - loss: 0.1389\n",
            "0.1388889104127884\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1389\n",
            "0.1388888955116272\n"
          ]
        }
      ]
    }
  ]
}